{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1641196298060,
     "user": {
      "displayName": "Seo in Seoul",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghwk1keCboZ5G83G8PzmpDMAtCmb94WL-59SAiJ=s64",
      "userId": "18324634975507226440"
     },
     "user_tz": -540
    },
    "id": "LHERlet9TZDN"
   },
   "outputs": [],
   "source": [
    "# *** version History \n",
    "# ------------------- Done -------------------------\n",
    "# ver1. : dafault -> metric = \"F1\"\n",
    "# ver2. : training metric -> \"AUC\"\n",
    "#   => Max_iter=3000 까지 수렴하지 않고 계속 학습되는 현상 발생 \n",
    "#      -> 5-fold 중에서, 3,4번째 제외하고 모두 끝이 안났음 \n",
    "#         -> 3, 4번째도, 2990번 즈음에야 Shrink 함 \n",
    "# ver3. : A 속성에 대한 사람==컨텐츠 bool 컬럼 추가 \n",
    "#   ver3.1 : E (순서형, 0~11)속성에 대한 비교 추가 + \n",
    "#   ver3.2 : E 속성 순서형에 대한, Binning 조정 \n",
    "# ver4. : 순서형 자료에 대한, numeric cols 반영 \n",
    "# ver4. : (*Validity 탐색 필요*, 열람시간대, 열람 요일별 판별력 반영) \n",
    "\n",
    "# ver6. : 똑같이 타고 들어가서, 속성 2개 짜리 별도로 모형에 추가해서 \n",
    "#   -> (모형 1: 여러 변수) & (모형 2: 두개 짜리 변수) 에 대한 앙상블해서 스코어 보기 \n",
    "#       => 0.70257 \n",
    "# ver7. : '열람일시'에 대한 변수 분할로 catboost 태우기 + ver6. 과 앙상블 하기 \n",
    "#       => 0.70285\n",
    "# ----------------- On Progress --------------------\n",
    "# ver8. : 파생변수 생성 + best-model-architecture로 적합 \n",
    "#   -> (생성한 속성 pool에서) Feature_selection 하기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "881lD0VJ_m2y"
   },
   "source": [
    "### Set Global variables   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1641196316807,
     "user": {
      "displayName": "Seo in Seoul",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghwk1keCboZ5G83G8PzmpDMAtCmb94WL-59SAiJ=s64",
      "userId": "18324634975507226440"
     },
     "user_tz": -540
    },
    "id": "i2SORQEJMcOT"
   },
   "outputs": [],
   "source": [
    "# DATA_PATH = \"/content/drive/MyDrive/dacon/job_care/data/\"\n",
    "# SUBMIT_PATH = \"/content/drive/MyDrive/dacon/job_care/submit/\"\n",
    "\n",
    "DATA_PATH = '../data/JobCare_data/'\n",
    "SUBMIT_PATH = '../submission/'\n",
    "SEED = 42    # Seed for reproducibility "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0yyXjHXJMJ1"
   },
   "source": [
    "### catboost install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11734,
     "status": "ok",
     "timestamp": 1641196328830,
     "user": {
      "displayName": "Seo in Seoul",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghwk1keCboZ5G83G8PzmpDMAtCmb94WL-59SAiJ=s64",
      "userId": "18324634975507226440"
     },
     "user_tz": -540
    },
    "id": "MQlCENZDe5Eg",
    "outputId": "657fdc4e-5534-4ac8-927d-c3800067a95a"
   },
   "outputs": [],
   "source": [
    "# !pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKOGpC7sTJLj"
   },
   "source": [
    "### Library import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1641196329222,
     "user": {
      "displayName": "Seo in Seoul",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghwk1keCboZ5G83G8PzmpDMAtCmb94WL-59SAiJ=s64",
      "userId": "18324634975507226440"
     },
     "user_tz": -540
    },
    "id": "mVGRBMjBTF_D",
    "outputId": "a50756db-0a0a-426f-b546-7d204869f166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- os: Linux-5.11.0-43-generic-x86_64-with-debian-bullseye-sid\n",
      "- python: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n",
      "[GCC 9.4.0]\n",
      "- pandas: 1.1.5\n",
      "- numpy: 1.19.5\n",
      "- sklearn: 1.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import random\n",
    "import math\n",
    "from typing import List ,Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn \n",
    "from sklearn.model_selection import StratifiedKFold , KFold\n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "from catboost import Pool, CatBoostClassifier \n",
    "from catboost import FeaturesData\n",
    "\n",
    "print(f\"- os: {platform.platform()}\")\n",
    "print(f\"- python: {sys.version}\")\n",
    "print(f\"- pandas: {pd.__version__}\")\n",
    "print(f\"- numpy: {np.__version__}\")\n",
    "print(f\"- sklearn: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "* Derivatives \n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset  \n",
    "data_dir = '../data/JobCare_data/'\n",
    "\n",
    "# path for Prediction.csv  \n",
    "subm_dir = '../submission/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_dir+'train.csv')\n",
    "test = pd.read_csv(data_dir+'test.csv')\n",
    "\n",
    "# Feature codes \n",
    "feature_D_code = pd.read_csv(data_dir+'속성_D_코드.csv')\n",
    "feature_H_code = pd.read_csv(data_dir+'속성_H_코드.csv')\n",
    "feature_L_code = pd.read_csv(data_dir+'속성_L_코드.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop 'Unnamed:5'\n",
    "# feature_D_code = feature_D_code.loc[:, feature_D_code.columns.str.contains('속성 D')]\n",
    "\n",
    "# Rename Columns\n",
    "D_code_map = {\n",
    "    '속성 D 코드': 'D_ALL', \n",
    "    '속성 D 세분류코드': 'D_DET', \n",
    "    '속성 D 소분류코드': 'D_SML', \n",
    "    '속성 D 중분류코드': 'D_MED', \n",
    "    '속성 D 대분류코드': 'D_LAG'\n",
    "}\n",
    "\n",
    "H_code_map = {\n",
    "    '속성 H 코드': 'H_ALL', \n",
    "    '속성 H 중분류코드': 'H_MED', \n",
    "    '속성 H 대분류코드': 'H_LAG'\n",
    "}\n",
    "\n",
    "L_code_map = {\n",
    "    '속성 L 코드': 'L_ALL', \n",
    "    '속성 L 세분류코드': 'L_DET', \n",
    "    '속성 L 소분류코드': 'L_SML', \n",
    "    '속성 L 중분류코드': 'L_MED', \n",
    "    '속성 L 대분류코드': 'L_LAG' \n",
    "}\n",
    "\n",
    "feature_D_code.columns = feature_D_code.columns.map(D_code_map)\n",
    "feature_H_code.columns = feature_H_code.columns.map(H_code_map)\n",
    "feature_L_code.columns = feature_L_code.columns.map(L_code_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime handling (str to dt)\n",
    "train['contents_open_dt'] = pd.to_datetime(train['contents_open_dt'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "test['contents_open_dt'] = pd.to_datetime(test['contents_open_dt'], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Train-set 'contents_open_dt' Min(): 2020-01-01 00:01:03,  Max():2020-11-30 23:59:56\n",
      " *  test-set 'contents_open_dt' Min(): 2020-12-01 00:00:07,  Max():2020-12-31 23:59:08\n"
     ]
    }
   ],
   "source": [
    "print(f\" * Train-set 'contents_open_dt' Min(): {train['contents_open_dt'].min()},  Max():{train['contents_open_dt'].max()}\")\n",
    "print(f\" *  test-set 'contents_open_dt' Min(): {test['contents_open_dt'].min()},  Max():{test['contents_open_dt'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Master tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create person & contents master tables\n",
    "person_train = train[train.columns[train.columns.str.contains('person_')]]\n",
    "contnt_train = train[train.columns[train.columns.str.contains('contents_')]]\n",
    "\n",
    "# >>> (~person_train.duplicated()).sum() \n",
    "# 300,177    # -> same w/ nunique(person_rn) \n",
    "# >>> (~(contnt_train.drop('contents_open_dt', axis=1).duplicated())).sum()\n",
    "# 283,393    # -> same w/ nunique(contents_rn) \n",
    "\n",
    "person_master = person_train.drop_duplicates(keep='first')\n",
    "contnt_master = contnt_train.drop('contents_open_dt', axis=1).drop_duplicates(keep='first')\n",
    "\n",
    "# Arrange columns' order \n",
    "person_master = pd.concat([person_master['person_rn'], person_master.loc[:, person_master.columns!='person_rn']], axis=1)\n",
    "contnt_master = pd.concat([contnt_master['contents_rn'], contnt_master.loc[:, contnt_master.columns!='contents_rn']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contnt_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "## * FeatureGenerator\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDerivativeFeatures(df):\n",
    "    # copy DataFrame  \n",
    "    data_df = df.copy()\n",
    "\n",
    "    # ------------------------------------\n",
    "    ### * 'contents_open_dt' -> split into dt.components  \n",
    "    ###    -> Col_prefix : \"open_dt_\"\n",
    "    # ------------------------------------\n",
    "    data_df['open_dt_quarter'] = data_df['contents_open_dt'].dt.quarter\n",
    "    data_df['open_dt_month'] = data_df['contents_open_dt'].dt.month\n",
    "    data_df['open_dt_week'] = data_df['contents_open_dt'].dt.week\n",
    "    data_df['open_dt_day'] = data_df['contents_open_dt'].dt.day\n",
    "    data_df['open_dt_weekday'] = data_df['contents_open_dt'].dt.weekday\n",
    "    # data_df['open_dt_day_name'] = data_df['contents_open_dt'].dt.day_name().str[:3]\n",
    "    data_df['open_dt_hour'] = data_df['contents_open_dt'].dt.hour\n",
    "    data_df['open_dt_minute'] = data_df['contents_open_dt'].dt.minute\n",
    "\n",
    "    # Columns' list \n",
    "    open_dt_cols = list(data_df.columns[data_df.columns.str.contains('open_dt_')])\n",
    "\n",
    "    # ------------------------------------\n",
    "    ### * All combination for each equal-attributes (: [a, a_1], [c], [j, j_1], [e])\n",
    "    ###    -> Col_prefix : \"derivatives_\"\n",
    "    # ------------------------------------\n",
    "    # * --- <str> ---\n",
    "    # person_attribute_a & contents_attribute_a\n",
    "    data_df['derivatives_a_a'] = data_df['person_attribute_a'].astype(str) + '-' + data_df['contents_attribute_a'].astype(str)\n",
    "\n",
    "    # person_attribute_a & person_attribute_a_1\n",
    "    data_df['derivatives_person_a_a_1'] = data_df['person_attribute_a'].astype(str) + '-' + data_df['person_attribute_a_1'].astype(str)\n",
    "\n",
    "    # person_prefer_c & contents_attribute_c\n",
    "    data_df['derivatives_c_c'] = data_df['person_prefer_c'].astype(str) + '-' + data_df['contents_attribute_c'].astype(str)\n",
    "\n",
    "    # contents_attribute_j & contents_attribute_j_1\n",
    "    data_df['derivatives_contents_j_j_1'] = data_df['contents_attribute_j'].astype(str) + '-' + data_df['contents_attribute_j_1'].astype(str) \n",
    "\n",
    "    # * --- <numeric> ---\n",
    "    # person_prefer_e & contents_attribute_e \n",
    "    data_df['derivatives_e_diff'] = data_df['person_prefer_e'] - data_df['contents_attribute_e']\n",
    "\n",
    "    # Columns' list \n",
    "    derivatives_cols = list(data_df.columns[data_df.columns.str.contains('derivatives_')])    # * All cols \n",
    "\n",
    "    derivatives_str_cols = derivatives_cols.copy()\n",
    "    derivatives_str_cols.remove('derivatives_e_diff')    # * str cols \n",
    "    derivatives_num_cols = ['derivatives_e_diff']    #  * numeric cols \n",
    "    \n",
    "    derivatives_cols_dict = {'derivatives_str_cols': derivatives_str_cols, 'derivatives_num_cols': derivatives_num_cols}\n",
    "    \n",
    "    # ------------------------------------\n",
    "    ### * '..._yn T/F Combination (: d, h)\n",
    "    ###    -> cols_suffix : \"_yn_comb\"\n",
    "    # ------------------------------------\n",
    "    # d/h_match_yn cols \n",
    "    d_yn_cols = data_df.columns[data_df.columns.str.contains('^d_')]\n",
    "    h_yn_cols = data_df.columns[data_df.columns.str.contains('^h_')]\n",
    "\n",
    "    # create T/F combination col \n",
    "    data_df['d_yn_comb'] = pd.concat([data_df[yn_col_].astype(str).str[0] for yn_col_ in d_yn_cols], axis=1).T.sum()\n",
    "    data_df['h_yn_comb'] = pd.concat([data_df[yn_col_].astype(str).str[0] for yn_col_ in h_yn_cols], axis=1).T.sum()\n",
    "\n",
    "    # expend more combination for 2-feats \n",
    "    data_df['d+h_yn_comb'] = 'D:' + data_df['d_yn_comb'] + '/H:' + data_df['h_yn_comb'] \n",
    "\n",
    "    # Columns' list \n",
    "    yn_comb_cols = list(data_df.columns[data_df.columns.str.contains('_yn_comb')])    # * All cols \n",
    "\n",
    "    # ------------------------------------\n",
    "    ### * \"person_prefer_{H, D}_{1, 2, 3}\" - Coherency \n",
    "    ###    -> cols_format : \"derivatives_{D_LAG, D_SML, H_MED, etc.}_sim\"\n",
    "    # ------------------------------------\n",
    "    # *feat- code handling \n",
    "    # -----------------------\n",
    "    feat_D = feature_D_code.astype(str).copy()\n",
    "    feat_H = feature_H_code.astype(str).copy()\n",
    "    feat_L = feature_L_code.astype(str).copy()\n",
    "\n",
    "    feat_D['D_MED'] = feat_D['D_LAG']+'-'+feat_D['D_MED']\n",
    "    feat_D['D_SML'] = feat_D['D_MED']+'-'+feat_D['D_SML']\n",
    "    feat_D['D_DET'] = feat_D['D_SML']+'-'+feat_D['D_DET']\n",
    "\n",
    "    feat_H['H_MED'] = feat_H['H_LAG']+'-'+feat_H['H_MED']\n",
    "\n",
    "    feat_L['L_MED'] = feat_L['L_LAG']+'-'+feat_L['L_MED']\n",
    "    feat_L['L_SML'] = feat_L['L_MED']+'-'+feat_L['L_SML']\n",
    "    feat_L['L_DET'] = feat_L['L_SML']+'-'+feat_L['L_DET']\n",
    "\n",
    "    # * \"derivatives_{attribute_class_lv}_sim\"\n",
    "    # -----------------------\n",
    "    # Bucket for derivatives \n",
    "    data_df_0 = data_df.copy()\n",
    "    derivatives_feat_df = [data_df_0]\n",
    "    derivatives_feat_cols_dict = {}    # Columns' dict \n",
    "\n",
    "    # * person_prefer_{d, h}_{1, 2, 3} Comparison cols \n",
    "    for feat_, feat_df, cat_lvs_ in zip([\"D\", 'H'], [feat_D, feat_H], [['D_LAG', 'D_MED', 'D_SML'], ['H_LAG', 'H_MED']]): \n",
    "        person_prefer_cols = data_df.columns[data_df.columns.str.contains(f'person_prefer_{feat_.lower()}')]\n",
    "        # cat_lvs = ['H_LAG', 'H_MED']\n",
    "\n",
    "        person_prefer_frame = []\n",
    "\n",
    "        # for cat_lv_ in cat_lvs: \n",
    "        for cat_lv_ in cat_lvs_: \n",
    "            # Mapping dictionary\n",
    "            cat_lv_mapper = {rec_[f'{feat_}_ALL'] : rec_[cat_lv_] for _, rec_ in feat_df[[f'{feat_}_ALL', cat_lv_]].iterrows()} \n",
    "            # Prepare the [prefer_{1, 2, 3}]\n",
    "            person_prefer_view = data_df[person_prefer_cols].astype(str).copy()\n",
    "            # feat - by target_level - converting  \n",
    "            person_prefer_cnvt = pd.concat([person_prefer_view[col_].map(cat_lv_mapper) for col_ in person_prefer_cols], axis=1)\n",
    "            # Bool for equal-class - <bool> \n",
    "            frames = [(person_prefer_cnvt.iloc[:, lf_]==person_prefer_cnvt.iloc[:, rt_]).rename(f\"derivatives_{cat_lv_}_{str(lf_+1)}{str(rt_+1)}\") for lf_, rt_ in [(0, 1), (0, 2), (1, 2)]]\n",
    "            person_prefer_lv_yn = pd.concat(frames, axis=1)\n",
    "\n",
    "            # Similarity Strength -<numeric> \n",
    "            person_prefer_lv_yn[f\"derivatives_{cat_lv_}_sim\"] = person_prefer_lv_yn.sum(axis=1)\n",
    "            person_prefer_frame.append(person_prefer_lv_yn)    # Append for \"a-feat-by-each-lvs\"\n",
    "\n",
    "        person_prefer_deriv_df = pd.concat(person_prefer_frame, axis=1)    # Concat for \"a-feat-by-all-lvs\" \n",
    "\n",
    "        # Columns' list \n",
    "        mask_for_num_ = person_prefer_deriv_df.columns.str.contains(\"_sim\")\n",
    "\n",
    "        derivatives_feat_cols_dict[f'{feat_}_bool_cols'] = list(person_prefer_deriv_df.columns[~mask_for_num_])\n",
    "        derivatives_feat_cols_dict[f'{feat_}_num_cols'] = list(person_prefer_deriv_df.columns[mask_for_num_])\n",
    "\n",
    "        derivatives_feat_df.append(person_prefer_deriv_df)\n",
    "\n",
    "    # * concat all-feat \n",
    "    data_df = pd.concat(derivatives_feat_df, axis=1)\n",
    "\n",
    "    # ------------------------------------\n",
    "    ### * {[Person-Content] suitability} & {D/H \"Preference-coherency\"} \n",
    "    ###    -> cols_suffix : \"_sim_yn_comb\"\n",
    "    # ------------------------------------\n",
    "\n",
    "    # d/h_match_yn cols \n",
    "    d_sim_cols = data_df.columns[data_df.columns.str.contains('D_..._sim', regex=True)]\n",
    "    h_sim_cols = data_df.columns[data_df.columns.str.contains('H_..._sim', regex=True)]\n",
    "\n",
    "    # T-F Combination \n",
    "    d_yn_comb_with_sim = pd.concat([data_df['d_yn_comb']+'_'+data_df[d_sim_col_].astype(str) for d_sim_col_ in d_sim_cols], axis=1)\n",
    "    h_yn_comb_with_sim = pd.concat([data_df['h_yn_comb']+'_'+data_df[h_sim_col_].astype(str) for h_sim_col_ in h_sim_cols], axis=1)\n",
    "\n",
    "    d_yn_comb_with_sim.columns = d_sim_cols+'_yn_comb'\n",
    "    h_yn_comb_with_sim.columns = h_sim_cols+'_yn_comb'\n",
    "\n",
    "    # append all  \n",
    "    data_df = pd.concat([data_df, d_yn_comb_with_sim, h_yn_comb_with_sim], axis=1)\n",
    "\n",
    "    # Columns' list \n",
    "    sim_yn_comb_cols = d_yn_comb_with_sim.columns.tolist() +  h_yn_comb_with_sim.columns.tolist() \n",
    "    \n",
    "    # ------------------------------------\n",
    "    #### * 'contents_attribute_l' -> split into all class-lv\n",
    "    ###    -> cols_format : \"contents_attribute_L_{LAG, MED}\"\n",
    "    # ------------------------------------\n",
    "    # convert to 'L_LAG' code\n",
    "    attr_l_mapper = {r_[1]['L_ALL']: r_[1]['L_LAG'] for r_ in feat_L[['L_ALL', 'L_LAG']].iterrows()}\n",
    "    data_df['contents_attribute_L_LAG'] = data_df['contents_attribute_l'].astype(str).map(attr_l_mapper)\n",
    "\n",
    "    # convert to 'L_MED' code \n",
    "    attr_l_mapper = {r_[1]['L_ALL']: r_[1]['L_MED'] for r_ in feat_L[['L_ALL', 'L_MED']].iterrows()}\n",
    "    data_df['contents_attribute_L_MED'] = data_df['contents_attribute_l'].astype(str).map(attr_l_mapper)\n",
    "\n",
    "    # Columns' list \n",
    "    attr_l_cols = ['contents_attribute_L_LAG', 'contents_attribute_L_MED']\n",
    "    \n",
    "    # ----------------------------------- \n",
    "    all_derivatives_cols_dict = {\n",
    "        \"open_dt_cols\": open_dt_cols, \n",
    "        \"derivatives_cols_dict\": derivatives_cols_dict, \n",
    "        \"yn_comb_cols\": yn_comb_cols, \n",
    "        \"derivatives_feat_cols_dict\": derivatives_feat_cols_dict, \n",
    "        \"sim_yn_comb_cols\": sim_yn_comb_cols, \n",
    "        \"attr_l_cols\": attr_l_cols\n",
    "    }\n",
    "    # ----------------------------------- \n",
    "    return(data_df, all_derivatives_cols_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501951, 35)\n",
      "(46404, 34)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kok/.local/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "train_data, deriv_cols = getDerivativeFeatures(train)\n",
    "test_data, _ = getDerivativeFeatures(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501951, 77)\n",
      "(46404, 76)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "* Baseline \n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    df:pd.DataFrame, is_train:bool=True, \n",
    "    # cols_merge:List[Tuple[str,pd.DataFrame]]=[], \n",
    "    # cols_equi:List[Tuple[str,str]]=[],\n",
    "    cols_drop:List[str] = [\"id\",\"person_prefer_f\",\"person_prefer_g\" ,\"contents_open_dt\"]\n",
    "):\n",
    "# )->Tuple[pd.DataFrame,np.ndarray]:\n",
    "    df = df.copy()\n",
    "    y_data = None\n",
    "    \n",
    "    if is_train:\n",
    "        y_data = df[\"target\"].to_numpy()\n",
    "        df = df.drop(columns=\"target\")\n",
    "\n",
    "    # for col, df_code in cols_merge:\n",
    "    #     df = merge_codes(df,df_code,col)\n",
    "\n",
    "    # * Bool -> int \n",
    "    cols = df.select_dtypes(bool).columns.tolist()\n",
    "    df[cols] = df[cols].astype(int)\n",
    "\n",
    "    # for col1, col2 in cols_equi:\n",
    "    #     df[f\"{col1}_{col2}\"] = (df[col1] == df[col2] ).astype(int)\n",
    "\n",
    "    # * Excluded cols \n",
    "    df = df.drop(columns=cols_drop)\n",
    "    \n",
    "    return(df, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1641196333654,
     "user": {
      "displayName": "Seo in Seoul",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghwk1keCboZ5G83G8PzmpDMAtCmb94WL-59SAiJ=s64",
      "userId": "18324634975507226440"
     },
     "user_tz": -540
    },
    "id": "r-EDv_4gTsGr"
   },
   "outputs": [],
   "source": [
    "# # 소분류 중분류 대분류 속성코드 merge 컬럼명 및 데이터 프레임 리스트\n",
    "# cols_merge = [\n",
    "#               (\"person_prefer_d_1\" , code_d),\n",
    "#               (\"person_prefer_d_2\" , code_d),\n",
    "#               (\"person_prefer_d_3\" , code_d),\n",
    "#               (\"contents_attribute_d\" , code_d),\n",
    "#               (\"person_prefer_h_1\" , code_h),\n",
    "#               (\"person_prefer_h_2\" , code_h),\n",
    "#               (\"person_prefer_h_3\" , code_h),\n",
    "#               (\"contents_attribute_h\" , code_h),\n",
    "#               (\"contents_attribute_l\" , code_l),\n",
    "# ]\n",
    "\n",
    "# # 회원 속성과 콘텐츠 속성의 동일한 코드 여부에 대한 컬럼명 리스트\n",
    "# cols_equi = [\n",
    "\n",
    "#     (\"contents_attribute_c\",\"person_prefer_c\"),\n",
    "#     (\"contents_attribute_e\",\"person_prefer_e\"),    # disable - Ver. 3-1 # Return in model 6 \n",
    "\n",
    "#     (\"person_prefer_d_1_attribute_d_s\" , \"contents_attribute_d_attribute_d_s\"),   # Additional \n",
    "#     (\"person_prefer_d_1_attribute_d_m\" , \"contents_attribute_d_attribute_d_m\"),   # Additional \n",
    "#     (\"person_prefer_d_1_attribute_d_l\" , \"contents_attribute_d_attribute_d_l\"),   # Additional \n",
    "#     (\"person_prefer_d_2_attribute_d_s\" , \"contents_attribute_d_attribute_d_s\"),\n",
    "#     (\"person_prefer_d_2_attribute_d_m\" , \"contents_attribute_d_attribute_d_m\"),\n",
    "#     (\"person_prefer_d_2_attribute_d_l\" , \"contents_attribute_d_attribute_d_l\"),\n",
    "#     (\"person_prefer_d_3_attribute_d_s\" , \"contents_attribute_d_attribute_d_s\"),\n",
    "#     (\"person_prefer_d_3_attribute_d_m\" , \"contents_attribute_d_attribute_d_m\"),\n",
    "#     (\"person_prefer_d_3_attribute_d_l\" , \"contents_attribute_d_attribute_d_l\"),\n",
    "\n",
    "#     # (\"person_prefer_h_1_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n",
    "#     # (\"person_prefer_h_2_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n",
    "#     # (\"person_prefer_h_3_attribute_h_p\" , \"contents_attribute_h_attribute_h_p\"),\n",
    "#     (\"person_prefer_h_1_attribute_h_m\" , \"contents_attribute_h_attribute_h_m\"),\n",
    "#     (\"person_prefer_h_1_attribute_h_l\" , \"contents_attribute_h_attribute_h_l\"),\n",
    "#     (\"person_prefer_h_2_attribute_h_m\" , \"contents_attribute_h_attribute_h_m\"),\n",
    "#     (\"person_prefer_h_2_attribute_h_l\" , \"contents_attribute_h_attribute_h_l\"),\n",
    "#     (\"person_prefer_h_3_attribute_h_m\" , \"contents_attribute_h_attribute_h_m\"),\n",
    "#     (\"person_prefer_h_3_attribute_h_l\" , \"contents_attribute_h_attribute_h_l\"), \n",
    "#     # Additional attr_'A' - in ver3. / in ver5.\n",
    "#     (\"contents_attribute_a\",\"person_attribute_a\")\n",
    "    \n",
    "# ]\n",
    "\n",
    "# 학습에 필요없는 컬럼 리스트\n",
    "cols_drop = [\"id\",\"person_prefer_f\",\"person_prefer_g\" ,\"contents_open_dt\", \"person_rn\", \"contents_rn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIIYFriJKjdc"
   },
   "source": [
    "# Preprocessing for Train/Test-set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2627,
     "status": "ok",
     "timestamp": 1641196370723,
     "user": {
      "displayName": "Seo in Seoul",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghwk1keCboZ5G83G8PzmpDMAtCmb94WL-59SAiJ=s64",
      "userId": "18324634975507226440"
     },
     "user_tz": -540
    },
    "id": "xeHhs6pzTxvz",
    "outputId": "4ebcd9b9-e18b-4a2e-ff82-5b9e6f52058d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((501951, 70), (501951,), (46404, 70))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train, y_train = preprocess_data(train_data, cols_merge=cols_merge, cols_equi=cols_equi, cols_drop=cols_drop)\n",
    "# x_test, _ = preprocess_data(test_data, is_train=False, cols_merge=cols_merge, cols_equi=cols_equi, cols_drop=cols_drop)\n",
    "x_train, y_train = preprocess_data(train_data, cols_drop=cols_drop)\n",
    "x_test, _ = preprocess_data(test_data, is_train=False, cols_drop=cols_drop)\n",
    "\n",
    "x_train.shape , y_train.shape , x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okHoydC0KoLs"
   },
   "source": [
    "# 범주형 컬럼 리스트(catboost 파라미터에 넣을 용도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.max_rows = 100\n",
    "# pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_l_match_yn                      2\n",
       "d_m_match_yn                      2\n",
       "d_s_match_yn                      2\n",
       "h_l_match_yn                      2\n",
       "h_m_match_yn                      2\n",
       "                                 ..\n",
       "derivatives_D_SML_sim_yn_comb    12\n",
       "derivatives_H_LAG_sim_yn_comb    12\n",
       "derivatives_H_MED_sim_yn_comb    12\n",
       "contents_attribute_L_LAG         21\n",
       "contents_attribute_L_MED         79\n",
       "Length: 70, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- \n",
    "### *** feature-group variation \n",
    "# ------------------------- \n",
    "### * Cardinality-based column grouping \n",
    "# 1. df.nunique().gt( 2 ) & df.nunique().le( 50 )\n",
    "# 2. df.nunique().eq( 2 ) \n",
    "# 3. df.nunique().gt( 50 ) \n",
    "\n",
    "### * Theme-based column grouping \n",
    "# 1. Only-1-in-the-group \n",
    "#    : \n",
    "# 2. Only-a-few-in-the-group \n",
    "#    : (by, Chi2-best-features() )\n",
    "# (3. every-features-and-by-the-group)\n",
    "#    : \n",
    "\n",
    "# ------------------------- \n",
    "### *** Model evaluation \n",
    "# ------------------------- \n",
    "# 1. Direct fitting -> compare F1-metric & Optimal-Threshold\n",
    "# 2. FeatureSelecton() -> Select Most Appropriate features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- \n",
    "### *** feature-group variation \n",
    "# ------------------------- \n",
    "### * Cardinality-based column grouping \n",
    "# 1. df.nunique().gt( 2 ) & df.nunique().le( 50 )\n",
    "# 2. df.nunique().eq( 2 ) \n",
    "# 3. df.nunique().gt( 50 ) \n",
    "# ------------------------- \n",
    "x_nuniq_ = x_train.nunique()\n",
    "\n",
    "### * Cardinality-based column grouping \n",
    "CB1_feats = x_nuniq_[x_nuniq_.gt(2)&x_nuniq_.le(50)].index.tolist() \n",
    "CB2_feats = x_nuniq_[x_nuniq_.eq(2)                ].index.tolist() \n",
    "CB3_feats = x_nuniq_[x_nuniq_.gt(50)               ].index.tolist() \n",
    "\n",
    "# * Universal Numeric features \n",
    "ordinal_feats = ['person_attribute_a_1', 'person_attribute_b', 'person_prefer_e', 'contents_attribute_e']\n",
    "\n",
    "# ordinal_feats -> to -> 'float' type \n",
    "x_train[ordinal_feats] = x_train[ordinal_feats].astype(np.float32)\n",
    "x_test[ordinal_feats] = x_test[ordinal_feats].astype(np.float32)\n",
    "\n",
    "# ----------- for iter_ ----------------------\n",
    "# # 범주형 속성에서, 수치형 속성 제거 \n",
    "# cat_features = list(set(cat_features) - set(ordinal_feats))\n",
    "\n",
    "# print(len(cat_features))    # <- 사용되는 범주형 변수 갯수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51s5rZfK99bJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDG1B-tzLCuM"
   },
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import FeaturesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep original set \n",
    "x_train_0 = x_train.copy()\n",
    "y_train_0 = y_train.copy()\n",
    "x_test_0  = x_test.copy()\n",
    "# ------------------------------- \n",
    "# * 더 이상 x_train, y_train... 은 원본아님 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkeqaUk4ggbe",
    "outputId": "56464477-4bfe-4a2b-f51d-9fab5943d4c4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " * feature-group : CB1_feats\n",
      " * The # of - Total feats: 35 = Cat feats: 31 + Num feats: 4 \n",
      "==================================================\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.6090565\ttest: 0.6102459\tbest: 0.6102459 (0)\ttotal: 34.1ms\tremaining: 5m 40s\n",
      "1000:\tlearn: 0.6442518\ttest: 0.6410028\tbest: 0.6410028 (1000)\ttotal: 30.5s\tremaining: 4m 34s\n",
      "2000:\tlearn: 0.6526265\ttest: 0.6438344\tbest: 0.6438344 (2000)\ttotal: 1m\tremaining: 4m 1s\n",
      "3000:\tlearn: 0.6589061\ttest: 0.6449666\tbest: 0.6449691 (2997)\ttotal: 1m 29s\tremaining: 3m 28s\n",
      "4000:\tlearn: 0.6647154\ttest: 0.6457004\tbest: 0.6457031 (3994)\ttotal: 1m 59s\tremaining: 2m 58s\n",
      "5000:\tlearn: 0.6701416\ttest: 0.6461352\tbest: 0.6461352 (5000)\ttotal: 2m 29s\tremaining: 2m 28s\n",
      "bestTest = 0.6461516023\n",
      "bestIteration = 5025\n",
      "Shrink model to first 5026 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.6085952\ttest: 0.6080912\tbest: 0.6080912 (0)\ttotal: 33.7ms\tremaining: 5m 36s\n",
      "1000:\tlearn: 0.6446623\ttest: 0.6393586\tbest: 0.6393586 (1000)\ttotal: 30.4s\tremaining: 4m 32s\n",
      "2000:\tlearn: 0.6529219\ttest: 0.6420531\tbest: 0.6420531 (2000)\ttotal: 1m\tremaining: 3m 59s\n",
      "3000:\tlearn: 0.6592483\ttest: 0.6430590\tbest: 0.6430590 (3000)\ttotal: 1m 29s\tremaining: 3m 27s\n",
      "4000:\tlearn: 0.6649438\ttest: 0.6437154\tbest: 0.6437187 (3993)\ttotal: 1m 58s\tremaining: 2m 57s\n",
      "bestTest = 0.643940419\n",
      "bestIteration = 4491\n",
      "Shrink model to first 4492 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.6088904\ttest: 0.6114515\tbest: 0.6114515 (0)\ttotal: 36.9ms\tremaining: 6m 9s\n",
      "1000:\tlearn: 0.6445773\ttest: 0.6404958\tbest: 0.6404958 (1000)\ttotal: 30.4s\tremaining: 4m 33s\n",
      "2000:\tlearn: 0.6529016\ttest: 0.6430050\tbest: 0.6430050 (2000)\ttotal: 1m\tremaining: 4m\n",
      "3000:\tlearn: 0.6590962\ttest: 0.6440693\tbest: 0.6440729 (2994)\ttotal: 1m 29s\tremaining: 3m 27s\n",
      "4000:\tlearn: 0.6646053\ttest: 0.6446653\tbest: 0.6446674 (3994)\ttotal: 1m 58s\tremaining: 2m 57s\n",
      "bestTest = 0.6449866891\n",
      "bestIteration = 4648\n",
      "Shrink model to first 4649 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.6089937\ttest: 0.6050351\tbest: 0.6050351 (0)\ttotal: 31.5ms\tremaining: 5m 14s\n",
      "1000:\tlearn: 0.6455570\ttest: 0.6369381\tbest: 0.6369381 (1000)\ttotal: 30.4s\tremaining: 4m 32s\n",
      "2000:\tlearn: 0.6537201\ttest: 0.6394689\tbest: 0.6394689 (2000)\ttotal: 60s\tremaining: 3m 59s\n",
      "3000:\tlearn: 0.6600861\ttest: 0.6405698\tbest: 0.6405700 (2999)\ttotal: 1m 29s\tremaining: 3m 28s\n",
      "4000:\tlearn: 0.6657687\ttest: 0.6411915\tbest: 0.6411946 (3985)\ttotal: 1m 58s\tremaining: 2m 57s\n",
      "5000:\tlearn: 0.6712093\ttest: 0.6417030\tbest: 0.6417030 (5000)\ttotal: 2m 28s\tremaining: 2m 28s\n",
      "6000:\tlearn: 0.6763502\ttest: 0.6420970\tbest: 0.6421093 (5955)\ttotal: 2m 58s\tremaining: 1m 58s\n",
      "bestTest = 0.6421093345\n",
      "bestIteration = 5955\n",
      "Shrink model to first 5956 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.6114791\ttest: 0.6110517\tbest: 0.6110517 (0)\ttotal: 36.1ms\tremaining: 6m 1s\n",
      "1000:\tlearn: 0.6446460\ttest: 0.6399454\tbest: 0.6399454 (1000)\ttotal: 30.3s\tremaining: 4m 32s\n",
      "2000:\tlearn: 0.6530876\ttest: 0.6426345\tbest: 0.6426364 (1998)\ttotal: 59.9s\tremaining: 3m 59s\n",
      "3000:\tlearn: 0.6593711\ttest: 0.6436777\tbest: 0.6436794 (2999)\ttotal: 1m 29s\tremaining: 3m 27s\n",
      "bestTest = 0.6441938281\n",
      "bestIteration = 3760\n",
      "Shrink model to first 3761 iterations.\n",
      "==================================================\n",
      " * feature-group : CB2_feats\n",
      " * The # of - Total feats: 24 = Cat feats: 24 + Num feats: 0 \n",
      "==================================================\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.5983262\ttest: 0.5992846\tbest: 0.5992846 (0)\ttotal: 6.51ms\tremaining: 1m 5s\n",
      "bestTest = 0.607126981\n",
      "bestIteration = 841\n",
      "Shrink model to first 842 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.5986313\ttest: 0.5981987\tbest: 0.5981987 (0)\ttotal: 6.84ms\tremaining: 1m 8s\n",
      "bestTest = 0.6065834761\n",
      "bestIteration = 852\n",
      "Shrink model to first 853 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.5979988\ttest: 0.6006759\tbest: 0.6006759 (0)\ttotal: 6.69ms\tremaining: 1m 6s\n",
      "bestTest = 0.6074720025\n",
      "bestIteration = 745\n",
      "Shrink model to first 746 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.5993198\ttest: 0.5953786\tbest: 0.5953786 (0)\ttotal: 5.01ms\tremaining: 50.1s\n",
      "bestTest = 0.6035501659\n",
      "bestIteration = 937\n",
      "Shrink model to first 938 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.5985668\ttest: 0.5984171\tbest: 0.5984171 (0)\ttotal: 6.8ms\tremaining: 1m 8s\n",
      "1000:\tlearn: 0.6107569\ttest: 0.6055075\tbest: 0.6055185 (991)\ttotal: 5.23s\tremaining: 47s\n",
      "bestTest = 0.6055184603\n",
      "bestIteration = 991\n",
      "Shrink model to first 992 iterations.\n",
      "==================================================\n",
      " * feature-group : CB3_feats\n",
      " * The # of - Total feats: 11 = Cat feats: 11 + Num feats: 0 \n",
      "==================================================\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.5912046\ttest: 0.5988499\tbest: 0.5988499 (0)\ttotal: 19.8ms\tremaining: 3m 17s\n",
      "1000:\tlearn: 0.6923060\ttest: 0.7076446\tbest: 0.7076446 (1000)\ttotal: 19.2s\tremaining: 2m 52s\n",
      "2000:\tlearn: 0.7006429\ttest: 0.7129772\tbest: 0.7129772 (2000)\ttotal: 37.8s\tremaining: 2m 31s\n",
      "3000:\tlearn: 0.7053745\ttest: 0.7144601\tbest: 0.7144606 (2998)\ttotal: 56.5s\tremaining: 2m 11s\n",
      "4000:\tlearn: 0.7093810\ttest: 0.7153424\tbest: 0.7153435 (3988)\ttotal: 1m 15s\tremaining: 1m 52s\n",
      "bestTest = 0.7156668007\n",
      "bestIteration = 4661\n",
      "Shrink model to first 4662 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.5922218\ttest: 0.5989079\tbest: 0.5989079 (0)\ttotal: 18ms\tremaining: 3m\n",
      "1000:\tlearn: 0.6918089\ttest: 0.7083865\tbest: 0.7083865 (1000)\ttotal: 19.1s\tremaining: 2m 51s\n",
      "2000:\tlearn: 0.7001557\ttest: 0.7132541\tbest: 0.7132560 (1996)\ttotal: 37.7s\tremaining: 2m 30s\n",
      "3000:\tlearn: 0.7050321\ttest: 0.7148120\tbest: 0.7148152 (2998)\ttotal: 56.4s\tremaining: 2m 11s\n",
      "4000:\tlearn: 0.7091086\ttest: 0.7155436\tbest: 0.7155437 (3999)\ttotal: 1m 15s\tremaining: 1m 52s\n",
      "5000:\tlearn: 0.7128181\ttest: 0.7160034\tbest: 0.7160037 (4987)\ttotal: 1m 33s\tremaining: 1m 33s\n",
      "bestTest = 0.7161085606\n",
      "bestIteration = 5254\n",
      "Shrink model to first 5255 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.5904790\ttest: 0.5961201\tbest: 0.5961201 (0)\ttotal: 18.4ms\tremaining: 3m 3s\n",
      "1000:\tlearn: 0.6920306\ttest: 0.7061984\tbest: 0.7062069 (999)\ttotal: 19.2s\tremaining: 2m 52s\n",
      "2000:\tlearn: 0.7004315\ttest: 0.7117340\tbest: 0.7117340 (2000)\ttotal: 37.9s\tremaining: 2m 31s\n",
      "3000:\tlearn: 0.7052199\ttest: 0.7132592\tbest: 0.7132592 (3000)\ttotal: 56.6s\tremaining: 2m 12s\n",
      "4000:\tlearn: 0.7093672\ttest: 0.7141466\tbest: 0.7141492 (3998)\ttotal: 1m 15s\tremaining: 1m 52s\n",
      "5000:\tlearn: 0.7130363\ttest: 0.7145463\tbest: 0.7145516 (4999)\ttotal: 1m 33s\tremaining: 1m 33s\n",
      "bestTest = 0.7146812081\n",
      "bestIteration = 5276\n",
      "Shrink model to first 5277 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.5935479\ttest: 0.5961727\tbest: 0.5961727 (0)\ttotal: 18.6ms\tremaining: 3m 6s\n",
      "1000:\tlearn: 0.6924838\ttest: 0.7064576\tbest: 0.7064576 (1000)\ttotal: 19.1s\tremaining: 2m 51s\n",
      "2000:\tlearn: 0.7005892\ttest: 0.7111675\tbest: 0.7111686 (1998)\ttotal: 37.9s\tremaining: 2m 31s\n",
      "3000:\tlearn: 0.7054423\ttest: 0.7126299\tbest: 0.7126299 (3000)\ttotal: 56.6s\tremaining: 2m 12s\n",
      "4000:\tlearn: 0.7094284\ttest: 0.7133645\tbest: 0.7133653 (3990)\ttotal: 1m 15s\tremaining: 1m 52s\n",
      "bestTest = 0.7136556506\n",
      "bestIteration = 4548\n",
      "Shrink model to first 4549 iterations.\n",
      "Learning rate set to 0.016489\n",
      "0:\tlearn: 0.5962356\ttest: 0.5986774\tbest: 0.5986774 (0)\ttotal: 17.8ms\tremaining: 2m 57s\n",
      "1000:\tlearn: 0.6923987\ttest: 0.7044263\tbest: 0.7044263 (1000)\ttotal: 19.1s\tremaining: 2m 51s\n",
      "2000:\tlearn: 0.7007298\ttest: 0.7094979\tbest: 0.7094979 (2000)\ttotal: 37.8s\tremaining: 2m 31s\n",
      "3000:\tlearn: 0.7055786\ttest: 0.7110820\tbest: 0.7110863 (2996)\ttotal: 56.5s\tremaining: 2m 11s\n",
      "4000:\tlearn: 0.7095886\ttest: 0.7119586\tbest: 0.7119595 (3996)\ttotal: 1m 15s\tremaining: 1m 52s\n",
      "5000:\tlearn: 0.7132683\ttest: 0.7124484\tbest: 0.7124485 (4999)\ttotal: 1m 33s\tremaining: 1m 33s\n",
      "bestTest = 0.712608099\n",
      "bestIteration = 5497\n",
      "Shrink model to first 5498 iterations.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# * 학습 파라미터 \n",
    "# ------------------------- \n",
    "is_holdout = False\n",
    "n_splits = 5\n",
    "iterations = 10000\n",
    "# iterations = 3600\n",
    "patience = 50\n",
    "\n",
    "cv = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# * 결과 담을 객체\n",
    "# ------------------------- \n",
    "scores_F1_total = {}\n",
    "scores_AUC_total = {}\n",
    "models_total = {}\n",
    "\n",
    "gp_names = [\"CB1_feats\", \"CB2_feats\", \"CB3_feats\"]\n",
    "gp_feats = [CB1_feats, CB2_feats, CB3_feats]\n",
    "\n",
    "for name_, col_group_ in zip(gp_names, gp_feats):\n",
    "    print(\"=\"*50)\n",
    "    print(f\" * feature-group : {name_}\")\n",
    "    \n",
    "    # * dataset partitioning  \n",
    "    # -------------------------- \n",
    "    x_train = x_train_0[col_group_].copy()\n",
    "    # x_test  = x_test_0[col_group_].copy()\n",
    "    y_train = y_train_0.copy()\n",
    "\n",
    "    # Score obj. initialization \n",
    "    # -------------------------- \n",
    "    # scores = []\n",
    "    scores_F1 = []\n",
    "    scores_AUC = []\n",
    "    models = []\n",
    "    \n",
    "    # * Assigning feature  \n",
    "    # --------------------------     \n",
    "    # 범주형 속성에서, 수치형 속성 제거 \n",
    "    cat_features = list(set(col_group_) - set(ordinal_feats))\n",
    "    print(f\" * The # of - Total feats: {len(col_group_)} = Cat feats: {len(cat_features)} + Num feats: {len(col_group_)-len(cat_features)} \") \n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # * Training with 5-folds cv \n",
    "    # -------------------------- \n",
    "    for tri, vai in cv.split(x_train):\n",
    "        # print(\"=\"*50)\n",
    "        preds = []\n",
    "        \n",
    "        # * baseline: metric = 'F1'\n",
    "        # model = CatBoostClassifier(iterations=iterations,random_state=SEED,task_type=\"GPU\",eval_metric=\"F1\",cat_features=cat_features,one_hot_max_size=4)\n",
    "\n",
    "        # * ver.2 : metric = 'AUC'\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=iterations, \n",
    "            random_state=SEED, \n",
    "            task_type=\"GPU\", \n",
    "            eval_metric=\"AUC\", \n",
    "            cat_features=cat_features, \n",
    "            one_hot_max_size=4, \n",
    "            # learning_rate=0.075\n",
    "            )\n",
    "\n",
    "        model.fit(x_train.iloc[tri], y_train[tri], \n",
    "                eval_set=[(x_train.iloc[vai], y_train[vai])], \n",
    "                early_stopping_rounds=patience,\n",
    "                verbose = 1000\n",
    "            )\n",
    "\n",
    "        models.append(model)\n",
    "        # scores_F1.append(model.get_best_score()[\"validation\"][\"F1\"])\n",
    "        scores_F1.append(f1_score(y_train[vai], model.predict(x_train.iloc[vai])))\n",
    "        scores_AUC.append(model.get_best_score()[\"validation\"][\"AUC\"])\n",
    "        \n",
    "        if is_holdout:\n",
    "            break    \n",
    "    \n",
    "    # * Append Results\n",
    "    scores_F1_total[f'{name_}'] = scores_F1\n",
    "    scores_AUC_total[f'{name_}'] = scores_AUC\n",
    "    models_total[f'{name_}'] = models\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_a2_yqqLF9M"
   },
   "source": [
    "# CV 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_F1_total\n",
    "# scores_AUC_total\n",
    "# models_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "LpumWH7BpaAT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      " * feature-group : CB1_feats\n",
      " * scores_F1 : [0.619827 0.618949 0.617994 0.617395 0.618025]\n",
      " * The Avg. of F1 scores : 0.618438\n",
      " * scores_AUC : [0.646152 0.64394  0.644987 0.642109 0.644194]\n",
      " * The Avg. of AUC scores : 0.644276\n",
      "==================================================\n",
      " * feature-group : CB2_feats\n",
      " * scores_F1 : [0.592573 0.593317 0.596608 0.593911 0.592455]\n",
      " * The Avg. of F1 scores : 0.593773\n",
      " * scores_AUC : [0.607127 0.606583 0.607472 0.60355  0.605518]\n",
      " * The Avg. of AUC scores : 0.60605\n",
      "==================================================\n",
      " * feature-group : CB3_feats\n",
      " * scores_F1 : [0.656975 0.656238 0.656526 0.651421 0.651017]\n",
      " * The Avg. of F1 scores : 0.654435\n",
      " * scores_AUC : [0.715667 0.716109 0.714681 0.713656 0.712608]\n",
      " * The Avg. of AUC scores : 0.714544\n"
     ]
    }
   ],
   "source": [
    "gp_names = [\"CB1_feats\", \"CB2_feats\", \"CB3_feats\"]\n",
    "\n",
    "for name_ in gp_names:\n",
    "    print(\"=\"*50)\n",
    "    print(f\" * feature-group : {name_}\")\n",
    "    \n",
    "    scores_F1 = scores_F1_total[name_].copy()\n",
    "    scores_AUC = scores_AUC_total[name_].copy()\n",
    "    # -------------------------------------------------\n",
    "    print(f\" * scores_F1 : {np.round(scores_F1, 6)}\")\n",
    "    print(f\" * The Avg. of F1 scores : {np.mean(scores_F1).round(6)}\")\n",
    "\n",
    "    print(f\" * scores_AUC : {np.round(scores_AUC, 6)}\")\n",
    "    print(f\" * The Avg. of AUC scores : {np.mean(scores_AUC).round(6)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NT9AExaEZRFQ"
   },
   "source": [
    "### 최적 Threshold 값 탐색 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<catboost.core.CatBoostClassifier at 0x7f4f69e8cdd0>,\n",
       " <catboost.core.CatBoostClassifier at 0x7f4f69f89e90>,\n",
       " <catboost.core.CatBoostClassifier at 0x7f4f69e88bd0>,\n",
       " <catboost.core.CatBoostClassifier at 0x7f4f69e836d0>,\n",
       " <catboost.core.CatBoostClassifier at 0x7f4f69f89f50>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for m_ in models_total: \n",
    "#     print(m_)\n",
    "    \n",
    "models_total['CB1_feats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_names = [\"CB1_feats\", \"CB2_feats\", \"CB3_feats\"]\n",
    "gp_feats = [CB1_feats, CB2_feats, CB3_feats]\n",
    "\n",
    "threshold_total = {}\n",
    "score_total = {}\n",
    "proba_total = {}\n",
    "\n",
    "thres_range = range(10, 90, 5)\n",
    "\n",
    "for name_, col_group_ in zip(gp_names, gp_feats):    \n",
    "    # * dataset partitioning  \n",
    "    x_train = x_train_0[col_group_].copy()\n",
    "    x_test  = x_test_0[col_group_].copy()\n",
    "    y_train = y_train_0.copy()\n",
    "    \n",
    "    models = models_total[name_].copy() \n",
    "    # -------------------------------------------------\n",
    "    thres_bucket = [] # * bucket for \"Best-Threshold\" for each fold\n",
    "    score_bucket = [] # * bucket for \"Best-F1\" for each fold\"\n",
    "    proba_bucket = [] # * bucket for \"test-pred_proba\" for each-fold\n",
    "    \n",
    "    # * Optimal Threshold Search - lv0 (threshold : from 0.1 to 0.85)\n",
    "    for i,(tri, vai) in enumerate(cv.split(x_train)):\n",
    "        pred_proba = models[i].predict_proba(x_train.iloc[vai])[:, 1]\n",
    "        sco_by_thres_ = [] \n",
    "        for thrs_ in thres_range: \n",
    "            threshold = thrs_/100\n",
    "            pred = np.where(pred_proba >= threshold , 1, 0)\n",
    "            score = f1_score(y_train[vai], pred)\n",
    "\n",
    "            sco_by_thres_.append(score)\n",
    "        thrs_lv0 = pd.Series(sco_by_thres_, index=thres_range).idxmax()\n",
    "    \n",
    "    # * Optimal Threshold Search - lv1 (threshold-from-lv0 +/- 0.1)\n",
    "        thres_range_lv1 = range((thrs_lv0-10)*10, (thrs_lv0+10)*10, 5)\n",
    "        sco_by_thres_ = []\n",
    "        for thrs_ in thres_range_lv1: \n",
    "            threshold = thrs_/1000\n",
    "            pred = np.where(pred_proba >= threshold , 1, 0)\n",
    "            score = f1_score(y_train[vai], pred)\n",
    "            sco_by_thres_.append(score)    # < F1-scores\n",
    "        \n",
    "        best_thres_ser = pd.Series(sco_by_thres_, index=thres_range_lv1)\n",
    "        # thrs_lv1 = best_thres_ser.idxmax()    # < best threshold\n",
    "        # f1_lv1 = best_thres_ser.max()         # < best F1 - by best threshold\n",
    "    \n",
    "    # * get_prob_of_testset\n",
    "        test_pred_proba = models[i].predict_proba(x_test)[:, 1]\n",
    "        \n",
    "        thres_bucket.append(np.round(best_thres_ser.idxmax()/1000, 3)) # < best threshold\n",
    "        score_bucket.append(best_thres_ser.max()) # < best F1 - by best threshold\n",
    "        proba_bucket.append(test_pred_proba) # < \"test-pred_proba\" for each-fold\n",
    "        \n",
    "    threshold_total[f\"{name_}\"] = thres_bucket\n",
    "    score_total[f\"{name_}\"] = score_bucket\n",
    "    proba_total[f\"{name_}\"] = proba_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V5W022B77ui"
   },
   "source": [
    "### threshold 정의 & best-F1 score 평균 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "AP1en3aq77ui"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Threshold for each variants: {'CB1_feats': 0.344, 'CB2_feats': 0.338, 'CB3_feats': 0.348}\n",
      "==================================================\n",
      " * feature-group : CB1_feats\n",
      " * scores_F1 - previous : [0.619827 0.618949 0.617994 0.617395 0.618025]\n",
      " * scores_F1 - adjusted : [0.683717 0.68235  0.681472 0.681878 0.679754]\n",
      " * The Avg. of F1 - previous : 0.6184  ->  adjusted : 0.6818\n",
      "==================================================\n",
      " * feature-group : CB2_feats\n",
      " * scores_F1 - previous : [0.592573 0.593317 0.596608 0.593911 0.592455]\n",
      " * scores_F1 - adjusted : [0.670466 0.670582 0.667999 0.669412 0.668046]\n",
      " * The Avg. of F1 - previous : 0.5938  ->  adjusted : 0.6693\n",
      "==================================================\n",
      " * feature-group : CB3_feats\n",
      " * scores_F1 - previous : [0.656975 0.656238 0.656526 0.651421 0.651017]\n",
      " * scores_F1 - adjusted : [0.70721  0.706155 0.705719 0.706063 0.704415]\n",
      " * The Avg. of F1 - previous : 0.6544  ->  adjusted : 0.7059\n"
     ]
    }
   ],
   "source": [
    "best_threshold_total = {name_: np.mean(thes_list_).round(4) for name_, thes_list_ in threshold_total.items()}\n",
    "print(f\" * Threshold for each variants: {best_threshold_total}\")\n",
    "\n",
    "# --------------------------------------------------- \n",
    "\n",
    "gp_names = [\"CB1_feats\", \"CB2_feats\", \"CB3_feats\"]\n",
    "\n",
    "for name_ in gp_names:\n",
    "    print(\"=\"*50)\n",
    "    print(f\" * feature-group : {name_}\")\n",
    "    scores_F1_prev = scores_F1_total[name_].copy()    \n",
    "    scores_F1_adjs = score_total[name_].copy()\n",
    "    # -------------------------------------------------\n",
    "    print(f\" * scores_F1 - previous : {np.round(scores_F1_prev, 6)}\")\n",
    "    print(f\" * scores_F1 - adjusted : {np.round(scores_F1_adjs, 6)}\")\n",
    "    print(f\" * The Avg. of F1 - previous : {np.mean(scores_F1_prev).round(4)}  ->  adjusted : {np.mean(scores_F1_adjs).round(4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PETOfwP77uj"
   },
   "source": [
    "### 최종 Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# 1 ) each feat_variants: => Mean(proba) \n",
    "gp_names = [\"CB1_feats\", \"CB2_feats\", \"CB3_feats\"]\n",
    "\n",
    "proba_by_variants = {n_: np.mean(proba_total[n_], axis=0) for n_ in gp_names} \n",
    "# ------------------------\n",
    "# >>> proba_by_variants \n",
    "# {'CB1_feats': array([0.45660201, 0.4630387 , 0.45793774, ..., 0.59621287, 0.69742711,\n",
    "#         0.63699568]),\n",
    "#  'CB2_feats': array([0.51995581, 0.43018918, 0.50929319, ..., 0.53231262, 0.63724508,\n",
    "#         0.62969929]),\n",
    "#  'CB3_feats': array([0.34450704, 0.31392938, 0.40045779, ..., 0.66538393, 0.66460025,\n",
    "#         0.71595655])}\n",
    "# ------------------------\n",
    "\n",
    "# 1-> pred) \n",
    "pred_by_variants = {n_: np.where(proba_by_variants[n_]>=best_threshold_total[n_], 1, 0) for n_ in gp_names} \n",
    "# ------------------------\n",
    "# >>> pred_by_variants\n",
    "# {'CB1_feats': array([1, 1, 1, ..., 1, 1, 1]),\n",
    "#  'CB2_feats': array([1, 1, 1, ..., 1, 1, 1]),\n",
    "#  'CB3_feats': array([0, 0, 1, ..., 1, 1, 1])}\n",
    "# ------------------------\n",
    "\n",
    "# 1 -> 2:proba -> pred ) \n",
    "proba = np.mean([proba_ for proba_ in proba_by_variants.values()], axis=0)\n",
    "best_threshold_ = np.mean([thres_ for thres_ in best_threshold_total.values()])\n",
    "pred = np.where(proba>=np.round(best_threshold_, 4), 1, 0)\n",
    "# print(f\" * best_threshold : {np.round(best_threshold_, 4)} \")\n",
    "# ------------------------\n",
    "# >>> proba\n",
    "# array([0.44035495, 0.40238575, 0.45589624, ..., 0.59796981, 0.66642415,\n",
    "#        0.66088384])\n",
    "# >>> pred\n",
    "# array([1, 1, 1, ..., 1, 1, 1])\n",
    "#  * best_threshold : 0.3433 \n",
    "# ------------------------\n",
    "\n",
    "# 1-> pred -> votting_pred) \n",
    "pred_sum = np.sum([pred_ for pred_ in pred_by_variants.values()], axis=0)\n",
    "pred_by_vote = np.where(pred_sum>=len(gp_names)/2, 1, 0)\n",
    "# ------------------------\n",
    "# >>> pred_by_vote\n",
    "# array([1, 1, 1, ..., 1, 1, 1])\n",
    "# ------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.456602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.463039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.457938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.563599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.582852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46399</th>\n",
       "      <td>46399</td>\n",
       "      <td>0.556573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46400</th>\n",
       "      <td>46400</td>\n",
       "      <td>0.500873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46401</th>\n",
       "      <td>46401</td>\n",
       "      <td>0.596213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46402</th>\n",
       "      <td>46402</td>\n",
       "      <td>0.697427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46403</th>\n",
       "      <td>46403</td>\n",
       "      <td>0.636996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46404 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0          0  0.456602\n",
       "1          1  0.463039\n",
       "2          2  0.457938\n",
       "3          3  0.563599\n",
       "4          4  0.582852\n",
       "...      ...       ...\n",
       "46399  46399  0.556573\n",
       "46400  46400  0.500873\n",
       "46401  46401  0.596213\n",
       "46402  46402  0.697427\n",
       "46403  46403  0.636996\n",
       "\n",
       "[46404 rows x 2 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(SUBMIT_PATH+f'sample_submission.csv')\n",
    "sample_submission['target'] = proba_by_variants['CB1_feats']\n",
    "sample_submission\n",
    "# proba_by_variants['CB1_feats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 제출 파일 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(SUBMIT_PATH+f'sample_submission.csv')\n",
    "\n",
    "gp_names = [\"CB1_feats\", \"CB2_feats\", \"CB3_feats\"] \n",
    "\n",
    "for name_ in gp_names: \n",
    "    # * save proba ) \n",
    "    subm_df = sample_submission.copy()\n",
    "    subm_df['target'] = proba_by_variants[name_]\n",
    "    subm_df.to_csv(SUBMIT_PATH+f'CatBoost_model8_{name_}_proba.csv', index=False)\n",
    "\n",
    "    # * save pred ) \n",
    "    subm_df = sample_submission.copy()\n",
    "    subm_df['target'] = pred_by_variants[name_]\n",
    "    subm_df.to_csv(SUBMIT_PATH+f'CatBoost_model8_{name_}_pred.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(SUBMIT_PATH+f'sample_submission.csv')\n",
    "\n",
    "name_total_ = 'CB'\n",
    "\n",
    "# * save proba ) \n",
    "subm_df = sample_submission.copy()\n",
    "subm_df['target'] = proba\n",
    "subm_df.to_csv(SUBMIT_PATH+f'CatBoost_model8_{name_total_}_proba.csv', index=False)\n",
    "\n",
    "# * save pred ) \n",
    "subm_df = sample_submission.copy()\n",
    "subm_df['target'] = pred\n",
    "subm_df.to_csv(SUBMIT_PATH+f'CatBoost_model8_{name_total_}_pred.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Submission History \n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Threshold for each variants: {'CB1_feats': 0.344, 'CB2_feats': 0.338, 'CB3_feats': 0.348}\n",
      "==================================================\n",
      " * feature-group : CB1_feats\n",
      " * scores_F1 - previous : [0.619827 0.618949 0.617994 0.617395 0.618025]\n",
      " * scores_F1 - adjusted : [0.683717 0.68235  0.681472 0.681878 0.679754]\n",
      " * The Avg. of F1 - previous : 0.6184  ->  adjusted : 0.6818              -> Test : 0.6818-894732\n",
      "==================================================\n",
      " * feature-group : CB2_feats\n",
      " * scores_F1 - previous : [0.592573 0.593317 0.596608 0.593911 0.592455]\n",
      " * scores_F1 - adjusted : [0.670466 0.670582 0.667999 0.669412 0.668046]\n",
      " * The Avg. of F1 - previous : 0.5938  ->  adjusted : 0.6693              -> Test : 0.6710-332843\t\n",
      "==================================================\n",
      " * feature-group : CB3_feats\n",
      " * scores_F1 - previous : [0.656975 0.656238 0.656526 0.651421 0.651017]\n",
      " * scores_F1 - adjusted : [0.70721  0.706155 0.705719 0.706063 0.704415]\n",
      " * The Avg. of F1 - previous : 0.6544  ->  adjusted : 0.7059              -> Test : 0.6941-091509\t\n",
      " \n"
     ]
    }
   ],
   "source": [
    "res_ = \"\"\"\n",
    "* Threshold for each variants: {'CB1_feats': 0.344, 'CB2_feats': 0.338, 'CB3_feats': 0.348}\n",
    "==================================================\n",
    " * feature-group : CB1_feats\n",
    " * scores_F1 - previous : [0.619827 0.618949 0.617994 0.617395 0.618025]\n",
    " * scores_F1 - adjusted : [0.683717 0.68235  0.681472 0.681878 0.679754]\n",
    " * The Avg. of F1 - previous : 0.6184  ->  adjusted : 0.6818              -> Test : 0.6818-894732\n",
    "==================================================\n",
    " * feature-group : CB2_feats\n",
    " * scores_F1 - previous : [0.592573 0.593317 0.596608 0.593911 0.592455]\n",
    " * scores_F1 - adjusted : [0.670466 0.670582 0.667999 0.669412 0.668046]\n",
    " * The Avg. of F1 - previous : 0.5938  ->  adjusted : 0.6693              -> Test : 0.6710-332843\t\n",
    "==================================================\n",
    " * feature-group : CB3_feats\n",
    " * scores_F1 - previous : [0.656975 0.656238 0.656526 0.651421 0.651017]\n",
    " * scores_F1 - adjusted : [0.70721  0.706155 0.705719 0.706063 0.704415]\n",
    " * The Avg. of F1 - previous : 0.6544  ->  adjusted : 0.7059              -> Test : 0.6941-091509\t\n",
    " \"\"\"\n",
    "\n",
    "print(res_)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[BaseModel]catboost+CV5Fold_threshold4e-1_Model03_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
